# R Analytics Module - Audit Report

**Date:** November 10, 2024  
**Reviewer:** Based on user feedback  
**Status:** ‚ö†Ô∏è **CRITICAL ISSUES FOUND**

---

## Executive Summary

R√† so√°t module R Analytics ph√°t hi·ªán **5 v·∫•n ƒë·ªÅ nghi√™m tr·ªçng** c·∫ßn fix ngay tr∆∞·ªõc khi deploy production. Module hi·ªán t·∫°i c√≥ ki·∫øn tr√∫c kh√¥ng nh·∫•t qu√°n, thi·∫øu b·∫£o m·∫≠t, v√† c√≥ nhi·ªÅu edge cases ch∆∞a x·ª≠ l√Ω.

### Risk Level: **HIGH** üî¥

---

## 1. ‚ö†Ô∏è CRITICAL: Ki·∫øn Tr√∫c Kh√¥ng Nh·∫•t Qu√°n

### V·∫•n ƒê·ªÅ:
`analysis_server.R` g·ªçi c√°c helper functions nh∆∞:
- `calculate_descriptive_stats()`
- `calculate_correlation_matrix()`
- `perform_linear_regression()`
- `perform_efa()`
- `perform_sem()`

**NH∆ØNG** c√°c functions n√†y KH√îNG ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!

### Hi·ªán Tr·∫°ng:
```r
# analysis_server.R line ~150
results$descriptive <- calculate_descriptive_stats(df, variables)
# ‚ùå ERROR: object 'calculate_descriptive_stats' not found
```

C√°c file trong `endpoints/` nh∆∞ `descriptive-stats.R`, `regression.R` ch·ªâ ƒë·ªãnh nghƒ©a helper functions nh∆∞ng KH√îNG ƒë∆∞·ª£c source v√†o `analysis_server.R`.

### Impact:
- ‚ùå **100% endpoints s·∫Ω l·ªói khi ch·∫°y**
- ‚ùå API kh√¥ng th·ªÉ ho·∫°t ƒë·ªông
- ‚ùå Blocking deployment

### Gi·∫£i Ph√°p:

#### Option A: Source Helper Files (Recommended)
```r
# Th√™m v√†o ƒë·∫ßu analysis_server.R
source("endpoints/descriptive-stats.R")
source("endpoints/regression.R")
source("endpoints/factor-analysis.R")
source("endpoints/sem.R")
```

#### Option B: Inline Helpers
Chuy·ªÉn t·∫•t c·∫£ helper functions v√†o `analysis_server.R`

#### Option C: Plumber Router
```r
# analysis_server.R
pr <- plumber$new()
pr$mount("/descriptive", plumber$new("endpoints/descriptive-stats.R"))
pr$mount("/regression", plumber$new("endpoints/regression.R"))
```

**Recommendation:** Option A (nhanh nh·∫•t, √≠t thay ƒë·ªïi)

---

## 2. üî¥ CRITICAL: Global State Kh√¥ng An To√†n

### V·∫•n ƒê·ªÅ:
```r
# D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u trong RAM
analysis_data <- new.env()
```

### R·ªßi Ro:
1. **M·∫•t d·ªØ li·ªáu khi restart** - Kh√¥ng c√≥ persistence
2. **Race condition** - Kh√¥ng c√≥ locking mechanism
3. **Memory leak** - Kh√¥ng c√≥ TTL ho·∫∑c cleanup
4. **Multi-process** - Kh√¥ng sync gi·ªØa c√°c process
5. **Ghi ƒë√® d·ªØ li·ªáu** - Concurrent requests c√≥ th·ªÉ conflict

### Impact:
- ‚ö†Ô∏è D·ªØ li·ªáu ph√¢n t√≠ch b·ªã m·∫•t
- ‚ö†Ô∏è K·∫øt qu·∫£ kh√¥ng nh·∫•t qu√°n
- ‚ö†Ô∏è Production kh√¥ng stable

### Gi·∫£i Ph√°p:

#### Short-term (Quick Fix):
```r
# Th√™m project_id key v√† TTL
analysis_data <- new.env()

store_data <- function(project_id, data) {
  key <- paste0("project_", project_id)
  analysis_data[[key]] <- list(
    data = data,
    timestamp = Sys.time(),
    ttl = 3600  # 1 hour
  )
}

get_data <- function(project_id) {
  key <- paste0("project_", project_id)
  stored <- analysis_data[[key]]
  
  if (is.null(stored)) return(NULL)
  
  # Check TTL
  if (difftime(Sys.time(), stored$timestamp, units = "secs") > stored$ttl) {
    rm(list = key, envir = analysis_data)
    return(NULL)
  }
  
  return(stored$data)
}

# Cleanup old data
cleanup_expired <- function() {
  keys <- ls(analysis_data)
  for (key in keys) {
    stored <- analysis_data[[key]]
    if (difftime(Sys.time(), stored$timestamp, units = "secs") > stored$ttl) {
      rm(list = key, envir = analysis_data)
    }
  }
}
```

#### Long-term (Recommended):
```r
# S·ª≠ d·ª•ng Supabase ho·∫∑c PostgreSQL
library(RPostgres)

con <- dbConnect(Postgres(),
  host = Sys.getenv("DB_HOST"),
  dbname = Sys.getenv("DB_NAME"),
  user = Sys.getenv("DB_USER"),
  password = Sys.getenv("DB_PASSWORD")
)

store_data <- function(project_id, data) {
  # Serialize data
  data_json <- toJSON(data)
  
  # Store in database
  dbExecute(con, 
    "INSERT INTO analysis_cache (project_id, data, created_at, expires_at)
     VALUES ($1, $2, NOW(), NOW() + INTERVAL '1 hour')
     ON CONFLICT (project_id) DO UPDATE SET data = $2, created_at = NOW()",
    params = list(project_id, data_json)
  )
}
```

---

## 3. üî¥ CRITICAL: CORS M·ªü Ho√†n To√†n

### V·∫•n ƒê·ªÅ:
```r
# analysis_server.R
options(plumber.cors = TRUE)
# Cho ph√©p T·∫§T C·∫¢ origins truy c·∫≠p
```

### R·ªßi Ro:
- ‚ö†Ô∏è B·∫•t k·ª≥ website n√†o c≈©ng c√≥ th·ªÉ g·ªçi API
- ‚ö†Ô∏è D·ªØ li·ªáu nh·∫°y c·∫£m c√≥ th·ªÉ b·ªã l·ªô
- ‚ö†Ô∏è CSRF attacks
- ‚ö†Ô∏è Kh√¥ng c√≥ rate limiting

### Gi·∫£i Ph√°p:
```r
# Gi·ªõi h·∫°n origins
#* @filter cors
function(req, res) {
  allowed_origins <- c(
    "https://ncskit.vercel.app",
    "https://your-domain.com",
    "http://localhost:3000"  # Dev only
  )
  
  origin <- req$HTTP_ORIGIN
  
  if (origin %in% allowed_origins) {
    res$setHeader("Access-Control-Allow-Origin", origin)
    res$setHeader("Access-Control-Allow-Methods", "GET, POST, OPTIONS")
    res$setHeader("Access-Control-Allow-Headers", "Content-Type, Authorization, X-API-Key")
    res$setHeader("Access-Control-Allow-Credentials", "true")
  }
  
  if (req$REQUEST_METHOD == "OPTIONS") {
    res$status <- 200
    return(list())
  }
  
  plumber::forward()
}

# Th√™m API Key authentication
#* @filter auth
function(req, res) {
  api_key <- req$HTTP_X_API_KEY
  valid_key <- Sys.getenv("ANALYTICS_API_KEY")
  
  if (is.null(api_key) || api_key != valid_key) {
    res$status <- 401
    return(list(error = "Unauthorized"))
  }
  
  plumber::forward()
}
```

---

## 4. ‚ö†Ô∏è HIGH: Edge Cases Ch∆∞a X·ª≠ L√Ω

### 4.1 Division by Zero (sd = 0)

**V·∫•n ƒë·ªÅ:**
```r
# Khi bi·∫øn h·∫±ng s·ªë (t·∫•t c·∫£ gi√° tr·ªã gi·ªëng nhau)
sd_value <- sd(x)  # = 0
z_score <- (x - mean(x)) / sd_value  # Division by zero!
```

**Locations:**
- `perform_data_health_check()` - normality test
- `detect_outliers()` - z-score calculation
- Correlation functions

**Fix:**
```r
# Guard cho sd = 0
calculate_z_scores <- function(x) {
  sd_val <- sd(x, na.rm = TRUE)
  
  if (is.na(sd_val) || sd_val == 0) {
    warning("Standard deviation is zero or NA. Returning NA.")
    return(rep(NA, length(x)))
  }
  
  return((x - mean(x, na.rm = TRUE)) / sd_val)
}

# Normality test guard
test_normality <- function(x) {
  # Remove NA
  x_clean <- x[!is.na(x)]
  
  # Check sample size
  if (length(x_clean) < 3) {
    return(list(test = "insufficient_data", p_value = NA))
  }
  
  # Check for constant values
  if (length(unique(x_clean)) == 1) {
    return(list(test = "constant_variable", p_value = NA))
  }
  
  # Check sd
  sd_val <- sd(x_clean)
  if (sd_val == 0) {
    return(list(test = "zero_variance", p_value = NA))
  }
  
  # Run test
  tryCatch({
    result <- shapiro.test(x_clean)
    return(list(test = "shapiro", p_value = result$p.value))
  }, error = function(e) {
    return(list(test = "error", p_value = NA, error = e$message))
  })
}
```

### 4.2 Sample Size Too Small

**V·∫•n ƒë·ªÅ:**
```r
shapiro.test(x)  # Requires n >= 3
ks.test(x)       # Requires n >= 2
```

**Fix:**
```r
# Add sample size checks
if (length(x_clean) < 3) {
  return(list(error = "Sample size too small for normality test (n < 3)"))
}
```

### 4.3 Factor Conversion

**V·∫•n ƒë·ªÅ:**
```r
# ANOVA/t-test c·∫ßn factor nh∆∞ng c√≥ th·ªÉ nh·∫≠n character
aov(y ~ group, data = df)  # Warning n·∫øu group l√† character
```

**Fix:**
```r
# Ensure factor
perform_anova <- function(data, dependent, independent) {
  # Convert grouping variables to factors
  for (var in independent) {
    if (!is.factor(data[[var]])) {
      data[[var]] <- as.factor(data[[var]])
    }
  }
  
  # Continue with analysis...
}
```

### 4.4 Outlier Index Mapping

**V·∫•n ƒë·ªÅ:**
```r
detect_outliers <- function(x) {
  x_clean <- x[!is.na(x)]
  outlier_indices <- which(abs(scale(x_clean)) > 3)
  return(outlier_indices)  # ‚ùå Index c·ªßa vector con, kh√¥ng ph·∫£i data g·ªëc
}
```

**Fix:**
```r
detect_outliers <- function(x) {
  # Keep track of original indices
  original_indices <- seq_along(x)
  valid_indices <- which(!is.na(x))
  x_clean <- x[valid_indices]
  
  # Detect outliers
  z_scores <- scale(x_clean)
  outlier_positions <- which(abs(z_scores) > 3)
  
  # Map back to original indices
  outlier_indices <- valid_indices[outlier_positions]
  
  return(list(
    indices = outlier_indices,
    values = x[outlier_indices],
    z_scores = z_scores[outlier_positions]
  ))
}
```

---

## 5. ‚ö†Ô∏è MEDIUM: Performance Issues

### 5.1 Heavy Bootstrap

**V·∫•n ƒë·ªÅ:**
```r
# Mediation analysis
mediate(model, sims = 1000)  # M·∫•t 10-30 gi√¢y
```

**Fix:**
```r
# Cho ph√©p client config
perform_mediation <- function(data, mediator, independent, dependent, 
                             bootstrap_sims = 1000) {
  # Validate sims
  if (bootstrap_sims > 5000) {
    warning("Bootstrap sims > 5000 may be slow. Consider reducing.")
  }
  
  # Run mediation
  result <- mediate(model, sims = bootstrap_sims)
  return(result)
}
```

### 5.2 CFA Bootstrap

**V·∫•n ƒë·ªÅ:**
```r
cfa(model, bootstrap = 1000)  # R·∫•t n·∫∑ng
```

**Fix:**
```r
# Default = FALSE, cho ph√©p client enable
perform_cfa <- function(data, model_syntax, 
                       bootstrap = FALSE,
                       bootstrap_samples = 1000) {
  if (bootstrap && bootstrap_samples > 2000) {
    warning("High bootstrap samples may take several minutes")
  }
  
  # Continue...
}
```

---

## 6. ‚ÑπÔ∏è LOW: Missing Tests & Documentation

### V·∫•n ƒê·ªÅ:
- Kh√¥ng c√≥ unit tests
- Modules placeholder (`clustering.R`, `sentiment.R`, `topics.R`) kh√¥ng ƒë∆∞·ª£c d√πng
- Kh√¥ng c√≥ documentation

### Gi·∫£i Ph√°p:

#### Add Tests:
```r
# tests/testthat/test-descriptive-stats.R
library(testthat)

test_that("calculate_descriptive_stats handles empty data", {
  df <- data.frame()
  result <- calculate_descriptive_stats(df)
  expect_equal(length(result), 0)
})

test_that("calculate_descriptive_stats handles constant variable", {
  df <- data.frame(x = rep(5, 10))
  result <- calculate_descriptive_stats(df, "x")
  expect_equal(result$numeric$sd_x, 0)
})

test_that("detect_outliers handles NA values", {
  x <- c(1, 2, 3, NA, 100)
  result <- detect_outliers(x)
  expect_true(5 %in% result$indices)  # Index 5 is outlier
})
```

#### Remove/Mark Placeholders:
```r
# modules/clustering.R
# ‚ö†Ô∏è EXPERIMENTAL - NOT READY FOR PRODUCTION
# TODO: Implement k-means, hierarchical clustering
```

---

## Priority Action Items

### üî¥ CRITICAL (Must Fix Before Deploy):

1. **Fix Helper Function Architecture** (2 hours)
   - [ ] Source helper files in `analysis_server.R`
   - [ ] Test all endpoints work
   - [ ] Verify no "object not found" errors

2. **Fix Global State** (4 hours)
   - [ ] Add project_id keying
   - [ ] Add TTL mechanism
   - [ ] Add cleanup function
   - [ ] Test concurrent requests

3. **Fix CORS & Security** (2 hours)
   - [ ] Restrict origins
   - [ ] Add API key authentication
   - [ ] Add rate limiting
   - [ ] Test security

### ‚ö†Ô∏è HIGH (Fix This Week):

4. **Add Edge Case Guards** (4 hours)
   - [ ] sd = 0 guards
   - [ ] Sample size checks
   - [ ] Factor conversion
   - [ ] Outlier index mapping

5. **Performance Optimization** (2 hours)
   - [ ] Configurable bootstrap sims
   - [ ] Add warnings for heavy operations
   - [ ] Test performance

### ‚ÑπÔ∏è MEDIUM (Fix Next Sprint):

6. **Add Tests** (8 hours)
   - [ ] Unit tests for all helpers
   - [ ] Integration tests for endpoints
   - [ ] Edge case tests

7. **Documentation** (4 hours)
   - [ ] API documentation
   - [ ] Function documentation
   - [ ] Usage examples

---

## Estimated Fix Time

- **Critical Issues:** 8 hours
- **High Priority:** 6 hours
- **Medium Priority:** 12 hours

**Total:** ~26 hours (3-4 days)

---

## Recommendations

### Immediate Actions (Today):
1. ‚úÖ Fix helper function sourcing
2. ‚úÖ Add basic guards for sd = 0
3. ‚úÖ Test all endpoints manually

### This Week:
1. Implement proper data storage
2. Fix CORS and add authentication
3. Add comprehensive edge case handling

### Next Sprint:
1. Write comprehensive tests
2. Add documentation
3. Performance optimization

---

## Conclusion

Module R Analytics c√≥ **ki·∫øn tr√∫c t·ªët** nh∆∞ng **implementation ch∆∞a ho√†n ch·ªânh**. C√°c v·∫•n ƒë·ªÅ critical c√≥ th·ªÉ fix trong 1-2 ng√†y. Sau khi fix, module s·∫Ω ·ªïn ƒë·ªãnh v√† ready for production.

**Current Status:** ‚ùå **NOT READY FOR PRODUCTION**  
**After Fixes:** ‚úÖ **PRODUCTION READY**

---

**Prepared by:** Kiro AI Assistant  
**Date:** November 10, 2024  
**Next Steps:** Fix critical issues before Vercel deployment
